[TOC]
### 非线性最小二乘的求解

#### 一、简单的最小二乘

问题可以描述成如下形式：
$$
x = \arg\min_{x}\frac{1}{2}\Vert f(x)\Vert_2^2
$$
可以通过给定初始值$x_0$之后，通过迭代算法，不断通过增量$\Delta x$去更新$x$的值，不断使目标函数下降。那么问题就变成了如何确定$\Delta x$使得目标函数不断下降。

#### 二、基本微积分知识补充

##### 2.1 多元函数微分知识

假设自变量为$n$维向量$x=[x_1,x_2,...,x_n]^T$，如果函数$f:R^n\rightarrow R$，则其梯度向量定义如下：
$$
g(x) = \nabla f(x) = (\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n})^T
$$
雅克比矩阵定义如下：
$$
J(x) = (\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n}) = g(x)^T
$$
`Hessian`矩阵如下：
$$
H(x) =  \nabla{x}{[g(x)]} = \nabla{x}\left [ \left( \frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\cdots,\frac{\partial f}{\partial x_n}\right)^T \right ] 
\\ = 
\begin{bmatrix}
\frac{\partial f}{\partial x_1 x_1} & \frac{\partial f}{\partial x_1 x_2} & \cdots & \frac{\partial f}{\partial x_1 x_n}\\ 
\frac{\partial f}{\partial x_2 x_1} & \frac{\partial f}{\partial x_2 x_2} & \cdots & \frac{\partial f}{\partial x_2 x_n}\\ 
\cdots & \cdots & \cdots & \cdots \\
\frac{\partial f}{\partial x_n x_1} & \frac{\partial f}{\partial x_n x_2} & \cdots & \frac{\partial f}{\partial x_n x_n}
\end{bmatrix}
$$
如果函数$f:R^n \rightarrow R^m$，那么其只有雅克比矩阵：
$$
J(x)=\nabla f(x) = \begin{bmatrix}
\frac{\partial f_1( x)}{\partial x_1} &  \frac{\partial f_1( x)}{\partial x_2}& \cdots & \frac{\partial f_1( x)}{\partial x_n}\\ 
\frac{\partial f_2( x)}{\partial x_1} &  \frac{\partial f_2( x)}{\partial x_2}& \cdots & \frac{\partial f_2( x)}{\partial x_n}\\ 
\vdots & \vdots &  \ddots& \vdots\\ 
\frac{\partial f_m( x)}{\partial x_1} &  \frac{\partial f_m( x)}{\partial x_2}& \cdots & \frac{\partial f_m( x)}{\partial x_n}\
\end{bmatrix}_{m \times n}=\begin{bmatrix}
g_1(x)^T\\ 
g_2(x)^T\\ 
\vdots \\ 
g_m(x)^T
\end{bmatrix}
$$


通常$f(x)$的泰勒展开可以写成如下形式：

$$
f(x+\Delta x) \approx f(x) + J(x)\Delta x + \Delta x^TH\Delta x^T
$$

##### 2.2 矩阵求导知识

$$
\begin{aligned}
Y &= A*X \Rightarrow\frac{dY}{dX} = A^T \\
Y &= X*A \Rightarrow\frac{dY}{dX} = A \\
Y &= A^T*X*B \Rightarrow \frac{dY}{dX} = A^T*B \\
Y &=  A^T*X^T*B \Rightarrow \frac{dY}{dX} = B*A^T \\
Y &= X^T*A*X  \Rightarrow \frac{dY}{dX} = A*X + A^T *X \\
Y &= X^T \Rightarrow \frac{dY}{dX}= I \\
Y &= X^T*A \Rightarrow \frac{dY}{dX} = A 
\end{aligned}
$$

#### 三、求解增量方程

##### 3.1 一阶梯度法

将目标函数$\Vert f(x)\Vert_2^2$在$x$附近进行泰勒展开：
$$
\Vert f(x+\Delta x) \Vert_2^2  \approx \Vert f(x) \Vert_2^2 + J(x)\Delta x + \frac{1}{2}\Delta x^T H\Delta x
$$
其中$J(x)$为__目标函数__的$Jacobian$矩阵，$H$为$Hessian$矩阵。

一阶梯度法只保留上式的一阶项，为使得目标函数下降的最快，可以选择:
$$
\Delta x^* = -J^T(x)
$$
即选择目标函数的负梯度方向，即为下降最快的方向，证明略。通常$-J(x)^T$只给出了更新的方向，所以一般还需要一个$\lambda$来给定步长。

__缺点__：过于贪心，容易走锯齿形路线，反而增加了迭代的次数。



##### 3.2 二阶梯度法(牛顿法)

$$
\Vert f(x+\Delta x) \Vert_2^2  \approx \Vert f(x) \Vert_2^2 + J(x)\Delta x + \frac{1}{2}\Delta x^T H\Delta x
$$

因为一般情况下都有$H^T=H$，所以上式右侧对$\Delta x$求导并且令导数为0，可以得到如下式子：
$$
\frac{d \Delta x ^TH \Delta x}{d \Delta x} = 2H\Delta x\\
H\Delta x = -J^T(x)
$$
__缺点__：需要计算$H$矩阵



##### 3.3 高斯牛顿法

将$f(x)$在$x$处进行一阶泰勒展开(上述的两个方法是对目标函数进行泰勒展开)：
$$
f(x+\Delta x) \approx f(x) + J(x)\Delta x
$$
$J(x)$为$f(x)$关于$x$的雅克比矩阵。

当前目标是寻找下降矢量$\Delta x$，使得$\Vert f(x+\Delta x)\Vert_2^2$ 达到最小，将上面的函数展开式代入到目标函数之中并展开可以得到如下式子：
$$
\Delta x^* = \arg\min_{\Delta x} \frac{1}{2}\Vert f(x)+J(x)\Delta x \Vert_2^2
$$
把上式展开可以得到：
$$
\begin{aligned}
\frac{1}{2}\Vert f(x) + J(x)\Delta x \Vert ^2 &= \frac{1}{2}(f(x)+J(x)\Delta x)^T(f(x)+J(x)\Delta x) \\
&=\frac{1}{2}(f(x)^T + \Delta x^TJ(x)^T)(f(x)+J(x)\Delta x) \\
&=\frac{1}{2}(\Vert f(x) \Vert_2^2  + f(x)^TJ(x)\Delta x  + \Delta x^TJ(x)^Tf(x)  + \Delta x^TJ(x)^TJ(x)\Delta x )
\end{aligned}
$$
对上式对$\Delta x$求导并令导数为0：
$$
(f(x)^TJ(x))^T + J(x)^Tf(x) + 2J(x)^TJ(x)\Delta x = 2J(x)^Tf(x) + 2J(x)^TJ(x)\Delta x = 0
$$
可以得到增量方程如下：
$$
J(x)^TJ(x)\Delta x = -J(x)^Tf(x)
$$
把左边系数定义为$H$，右边系数定义为$g$：
$$
H\Delta x = g
$$
可见，对比牛顿法，高斯牛顿法使用$J^TJ$作为$Hessian$矩阵的近似，省去了直接计算$Hessian$矩阵的过程。



__缺点__：原则上$H$的近似矩阵为可逆矩阵(并且是正定，正定一定可逆)，但是实际数据计算出来的$J^TJ$只有半正定，所以使得增量的稳定性较差，导致算法不收敛。



##### 3.4 Levenberg-Marquart算法

为$\Delta x$添加一个信赖区域，根据近似模型与实际函数之间的差异确定，最终的增量更新方程为：
$$
(H + \lambda I) \Delta x = g
$$

+ 当$\lambda$比较小的时候，$H$占主要地位，该方法接近于高斯牛顿法
+ 当$\lambda$比较大的时候，$\lambda I$占主要地位，该方法接近于一阶梯度法