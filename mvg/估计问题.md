## 估计问题

[TOC]

### 一、基本概念

​	通常，我们通常会更具已有的测量去估计某些我们需要的状态量。在计算机视觉中，比如；两幅图像上的对应点集去计算一幅图像到另外一幅图像点的2D射影变换；在已知两幅图片中点的对应关系去估计两视图之间的基本矩阵；根据空间中的3D点集和对应的相机图像平面上的2D点去估计3D点到2D点的射影映射关系。

​	对于2D射影变换$x_i  \leftrightarrow x_i'$ ，求解使得所有的$i$都满足的$Hx_i=x_i'$中的$3\times 3$矩阵$H$，首先介绍几个概念：

+ **测量数**:我们考虑这样一个问题：计算射影变换$H$最少需要多少的点对$x_i \leftrightarrow x_i′$。对于射影变换矩阵$H$，这是一个具有9个元素的矩阵，对于齐次表示，我们可以忽略一个尺度因子，从而这个2D射影变换的自由度为8。考虑点到点的对应关系的约束，第一幅图像上的点$x_i$映射到另外一幅视图上的点$x_i'$，这些2D点的自由度是2，尽管都是以$3\times1$齐次坐标表示，不考虑尺度因子他们的自由度仍为2。我们可以看到一对点可以提供两个约束方程（分别是$x$和$y$坐标各一个方程），所以约束一个自由度为8的矩阵$H$至少需要4组点对。 
+ **近似解** : 如果我们只给出4组对应点，那么可以得到$H$的精确解，也称为**最小配置解**。这种解定义了鲁棒估计（RANSAC）中所需的子集大小。但是点的测量往往会有噪声，如果我们给定多于4组解，那么这组数据可能不和任何一个射影变换完全符合，因此我们需要找到最适合这组数据的变换。通常会使用最小化某个代价函数来求得这个变换$H$。
+ **黄金标准算法**: 通常会存在一个最优的代价函数，也就是在一定的假设下，使得代价函数的最小值对应的$H$是最好的估计。计算该代价函数最小的算法则称为“黄金标准”算法。



### 二、直接线性变换

​	对于变换$H$，其完成映射$x_i \leftrightarrow x_i'$ ，由于都是齐次表示，所以无法直接用等号来表达该方程，所以可以利用向量叉乘来写成一个等式：
$$
x_i' \times Hx_i=0
$$
​	把$H$的第$j$行记做$h^{jT}$，那么：
$$
H\mathbf x_i=\begin{pmatrix}
\mathbf h^{1T}\mathbf x_i \\
\mathbf h^{2T}\mathbf x_i \\
\mathbf h^{3T}\mathbf x_i
\end{pmatrix}
$$
​	记$\mathbf x_i'=(x_i',y_i',w_i')^T$，叉乘等式可以表示为：
$$
\mathbf x_i' \times H\mathbf x_i=\begin{pmatrix}
y_i'\mathbf h^{3T}\mathbf x_i-w_i'\mathbf h^{2T}\mathbf x_i\\
w_i'\mathbf h^{1T}\mathbf x_i-x_i'\mathbf h^{3T}\mathbf x_i\\
x_i'\mathbf h^{2T}\mathbf x_i-y_i'\mathbf h^{1T}\mathbf x_i
\end{pmatrix}
$$
​	因为对于$j=1,2,3$，$\mathbf h^{jT}\mathbf x_i=\mathbf x_i^T\mathbf h^j$皆成立，从而我们可以给出关于$H$的三个方程，并写成如下形式：
$$
\begin{bmatrix}
\mathbf 0^T & -w_i'\mathbf x_i^T & y_i'\mathbf x_i^T \\
w_i'\mathbf x_i^T &\mathbf 0^T &  -x_i'\mathbf x_i^T \\
-y_i'\mathbf x_i^T & x_i'\mathbf x_i^T & \mathbf 0^T \\
\end{bmatrix}
\begin{bmatrix}
\mathbf h^1 \\
\mathbf h^2 \\
\mathbf h^3
\end{bmatrix}=\mathbf 0\tag{*}
$$
​	这些方程都有$A_i\mathbf{h}=0$的形式，其中$A_i$是$3\times9$的矩阵，$\mathbf{h}$是矩阵$H$的元素组成的9维向量:
$$
\begin{align}
\mathbf h =\begin{bmatrix}
\mathbf h^1 \\
\mathbf h^2 \\
\mathbf h^3
\end{bmatrix} &&
H=\begin{bmatrix}
h_1 & h_2 & h_3 \\
h_4 & h_5 & h_6 \\
h_7 & h_8 & h_9 
\end{bmatrix} 
\end{align}
$$
​	其中$h_i$是$\mathbf{h}$的第$i$个元素。在式子(∗)中有三个方程，其实只有两个是线性独立的（当第一行乘以$x_i'$加上第二行乘以$y_i'$就可以得到第三行所以通常会舍弃第三行等式，从而变成为：
$$
\begin{bmatrix}
\mathbf 0^T & -w_i'\mathbf x_i^T & y_i'\mathbf x_i^T \\
w_i'\mathbf x_i^T &\mathbf 0^T &  -x_i'\mathbf x_i^T \\
\end{bmatrix}
\begin{bmatrix}
\mathbf h^1 \\
\mathbf h^2 \\
\mathbf h^3
\end{bmatrix}=\mathbf 0
$$
记做：
$$
A_i\mathbf{h} = 0
$$
​	这里的$A_i$是一个$2\times 9$的矩阵，如果其中有一个点为理想点，即$w_i'=0$，则两个等式退化成一个等式，这时第三个等式不可以忽略。通常情况下，点$x_i$和点$x_i'$是齐次坐标，我们可以取最后一个参数为1，那么前两个参数也就是图像中实际测量的坐标。我们可以看到不论是使用3个等式还是2个等式，$A_i$的秩都是2，对应四组点我们得到的方程组$A\mathbf{h}=0$中系数矩阵$A$不论是$12 \times 9$还是$8 \times 9$的矩阵，它的秩都为8，所以有一维零空间，也就是存在一个非零尺度因子意义的确定解$\mathbf{h}$，一般会取约束$||\mathbf{h}|| =1$。

​	如果给定的点对$x_i \leftrightarrow x_i'$大于四组，那么方程组$A\mathbf{h}=0$是**超定**的。理想情况下，测量的图像坐标都是精确的，那么$A$的秩依旧是8，则存在精确解。而实际上，图像坐标的测量往往是有误差的（存在噪声），因此，除了零解外，超定方程不存在精确解。所以我们去找一个近似解，使得在某一代价函数下最小的$\mathbf{h}$的值。通常，会使用最小化范数$||A\mathbf{h}||$来找到这个近似解，并且增加约束$||\mathbf{h}||=1$，这也就等价于最小化$||A\mathbf{h}||/||\mathbf{h}||$。使用SVD分解的方法来求，那么该最小化问题的解就是$A^TA$最小特征值对应的（单位）特征矢量，也就是$A$最小奇异值对应的单位奇异矢量（$V$的最后一列矢量）。 

　　对应的**DLT算法**可以归纳如下：

> 1. 给定$n(\ge4)$组2D到2D的对应点{$x_i \leftrightarrow x_i'$} 
> 2. 由式子(*)确定矩阵A,将n个$2\times9$的矩阵组成$2n\times9$的矩阵A
> 3. 求A的SVD分解$A=UDV^T$，对应的最小奇异值的单位奇异向量就是解$\mathbf{h}$
> 4. 将$\mathbf{h}$转换成矩阵$H$



### 三、变换不变性和归一化

​	DLT算法是通过最小化矩阵范数的方式来求解。但是图像坐标有时会取在图像的左上角，有时会取在图像的中心，或者图像坐标的尺度有所变换，那么DLT求得的解会不会变化？

​	用代数形式进行讨论，图像点的坐标$x$被变换成$\hat{x}=Tx$，另外一张图像点的坐标$x'$变换成$\hat{x'}=T'x'$，这里的$T,T'$是$3\times3$变换矩阵，代入$x'=Hx$中得到$\hat{x'}=T'HT^{-1}\hat{x}$，这里的变换$\hat{H}=T'HT^{-1}$，就是对应$\hat{x}\leftrightarrow \hat{x'}$的变换矩阵，这里把点$x$映射到$x'$的另一种方式为

> 1. 根据$\hat{x}= Tx,\hat{x'}=T'x'$，变换图像坐标
> 2. 由对应点$\hat{x} \leftrightarrow \hat{x'}$，求得变换$\hat{H}$
> 3. 令$H=T'^{-1}\hat{H}T$

​	我们需要知道算法的结果是否与变换$T,T'$有关，MVG(P105)对相关内容进行了讨论，由于DLT求解的时候需要令$||H||=1$的约束，对应的变换后的坐标求解时，若选取$||\hat{H}||=1$，则导致求得的解不同。所以会存在某些变换求出的结果会比别的变换好，所以通常会采用**归一化变换(Normalizing transformations )**，优点如下：

+ 提升精度
+ 对任何尺度缩放和坐标原点的选择不变

归一化的其中一种手段是采用**各向同性缩放(Isotropic scaling )**：

​	先对每一幅图像中的坐标进行平移，使得点集的形心移至原点；然后对坐标系进行缩放，使得点$x=(x,y,w)^T$中各个坐标$x,y,w$总体上有一样的均值（之前可能是$\sum x:\sum y: \sum w = a_1 : a_2 : a_3$，同向缩放先使得$\sum x:\sum y: \sum w = 1 : 1: 1$)。最后，选择相同的缩放因子使得点$\mathbf{x}$到原点的平均距离为$\sqrt{2}$，也就是意味着平均点为$(1,1,1)^T$。概括起来：

> 1. 对点进行平移使得其形心位于原点。
> 2. 对点进行缩放使得它们到原点的距离为$\sqrt{2}$
> 3. 对两幅图像都做上述变换

​	所以求2D单应的**归一化DLT算法**变成如下形式：

> 1. 给定$n(\ge4)$组2D到2D的对应点{$x_i \leftrightarrow x_i'$} 
> 2. 归一化$\mathbf{x}$ : 将特征点进行只有平移和缩放的相似变换$T$，变换点$x_i$得到对应的点$\hat{x_i}$，使得变换后的点集的形心位于原点$(0,0)^T$，并且他们到原点的平均距离为$\sqrt{2}$
> 3. 归一化$\mathbf{x'}$：同上，通过相似变换$T'$，把点$x_i'$变换到点$\hat{x_i'}$，对应的相似变换为$T'$
> 4. 从归一化恢复原始的$H=T'^{-1}\hat{H}T$

​	同样我们也可以选择**非各向同性缩放(Non-isotropic scaling)**，首先和各向同性一样，把点集的形心平移到原点，然后对点的坐标进行缩放使得两个**主矩都为1**(对于各向同性缩放，其两个主矩不一定为1，只是到原点的距离为$\sqrt{2}$。

​	OpenCV中的8点法使用了各向同性缩放，而OpenCV求解Homography以及ORB_SLAM2中求解Fundamental和Homography都用到了非各向同性缩放：

- [OpenCV findFundamentalMat的8点算法](https://github.com/opencv/opencv/blob/3.1.0/modules/calib3d/src/fundam.cpp#L548)

- [OpenCV findHomography](https://github.com/opencv/opencv/blob/3.1.0/modules/calib3d/src/fundam.cpp#L113)

- [ORB-SLAM2 初始化中的归一化函数](https://github.com/raulmur/ORB_SLAM2/blob/master/src/Initializer.cc#L749)

   我们必须记住的一点是：**数值归一化在DLT算法中是必须的步骤，并非可有可无的。**	



### 四、鲁棒估计(RANSAC)

​	上述的分析中，我们都假设所给点集$\{x_i \leftrightarrow x′_i\}$的唯一误差来源于点的位置测量，然而在实际情况中，往往会存在点对的错匹配，这样的错匹配会给结果带来严重的偏差。所以，我们希望能从给定的点集中选出正确的点对，用这些正确的匹配数据去估计我们的模型，这样的数据的称为**内点（inliers ）**，而那些错匹配的数据称为**外点（outliers）**。这样通过内点去估计模型的方法称为**鲁棒估计（robust estimation ）**，因为这样的估计是对所给集合中的**野值**是能够忍受的。一个非常成功并且被普遍使用的鲁棒估计算法为——**随机采样一致性算法（Random sample consensus ，RANSAC ）**。RANSAC算法能够处理大比例的野值，其基本算法如下： 

> 1. 给定一个包含外点的集合$S$
> 2. 随机在集合$S$中随机选取$n$个数据点作为一个样本去估计模型
> 3. 根据计算出的模型，判断其余点是否在**距离阈值**$t$之内，如果是则这些点构成的点集$S_i$称为**采样一致集**，并定义为$S$的内点。
> 4. 如果$S_i$的数目大于某一个阈值$T$，则用内点集$S_i$重新估计模型并且结束
> 5. 否则重复步骤2-4，直到达到最大实验次数$N$
> 6. 经过最大的试验次数$N$则结束，并用选择最大一致集$S_i$，用$S_i$重新估计的模型作为输出

​	在这里我们需要考虑如下问题：

#### 1. 什么是距离阈值？

​	我们希望选定的距离阈值$t$使得在阈值内的点为内点的概率是$\alpha$。该计算需要知道内点到模型距离的概率分布，实际上通常使用经验选取。但是，如果假定测量误差为零均值和标准差为$\sigma$的高斯分布，那么$t$的值可以计算出来。因此，点的距离的平方$d_\bot^2$是高斯变量的平方并且服从一个自由度为$m$的$\chi^2_m$分布：
$$
x_1,x_2,\dots,x_m\sim N(0,\sigma^2)\\
\Rightarrow x_1^2+x_2^2+\dots+x_m^2\sim \chi_m^2
$$
​	其中$m$等于模型的余维度(codimension)。由于误差$d_x,d_y\sim N(0,\sigma^2)$，模型若为直线，由于测量的距离是点$(x,y)$到线$ax+by+c=0$的距离$d_\bot^2=||ax+by+c||^2/(a^2+b^2 ) \sim dx^2\sim\chi_1^2$，所以余维度为1。如果模型为一个点，由于距离的平方和是$x,y$测量误差的平方和$d_\bot^2=dx^2+dy^2\sim \chi_2^2$，其余维度为2。随机变量$\chi_m^2$的值小于$k^2$的概率由累积$\chi^2$分布$F_m(k^2)=\int_0^{k^2}\chi_m^2(\zeta)d\zeta$给出。由该累加分布可知：
$$
\begin{align}
\begin{cases}
\text{内点} & d_\bot^2<t^2\\
\text{外点} & d_\bot^2\ge t^2
\end{cases}
&& t^2=F_M^{-1}(\alpha)\sigma^2
\end{align}
$$
​	通常$\alpha$取0.95，即点为内点的概率为95%。它表明点被错误排斥的概率仅是次数的5%。下表是各个模型在$\alpha=0.95$时，$t$的值：

| 余维度 | 模型             | $t^2$          |
| ------ | ---------------- | -------------- |
| 1      | 直线、基本矩阵   | $3.84\sigma^2$ |
| 2      | 单应，摄像机矩阵 | $5.99\sigma^2$ |
| 3      | 三焦点张量       | $7.81\sigma^2$ |



#### 2. 采样多少次合适？

​	我们没有必要尝试所有的可能样本，只需要让采样次数$N$足够大，以保证由$s$个点组成的随机样本中至少有一次没有野值的概率为$p$，通常$p$取99%。假定$w$是任意选择的数据点为内点的概率，那么$\epsilon=1-w$是其为野值的概率。那么至少选择$N$次，其中$(1-w^s)^N=1-p$，从而
$$
N=log(1-p)/log(1-(1-\epsilon)^s)
$$
​	当$p=99\%$的时候，给出$N$的值如下表：

| $s / \epsilon $ | $5\%$ | $10\%$ | $20\%$ | $25\%$ | $30\%$ | $40\%$ | $50\%$ |
| --------------- | ----- | ------ | ------ | ------ | ------ | ------ | ------ |
| 2               | 2     | 3      | 5      | 6      | 7      | 11     | 17     |
| 3               | 3     | 4      | 7      | 9      | 11     | 19     | 35     |
| 4               | 3     | 5      | 9      | 13     | 17     | 34     | 72     |
| 5               | 4     | 6      | 12     | 17     | 26     | 57     | 146    |
| 6               | 4     | 7      | 16     | 24     | 37     | 97     | 293    |
| 7               | 4     | 8      | 20     | 33     | 54     | 163    | 588    |
| 8               | 5     | 9      | 26     | 44     | 78     | 272    | 1177   |

**注意：**

+ 采样次数与野值所占比例有关而不是与野点数目有关。因此采样的计算代价即使在野值数目很大的情况下也能接受
+ (对于给定的$\epsilon$和$p$）样本数随最小子集的增大而增大。有人认为采用大于最小子集的子集，然后用最小二乘去拟合会有好处，这样会更加精确地反应模型。但是由于增加采样数据而导致的计算代价一般会大大超过增加测量子集大小带来的可能好处。

通常数据中的野值所占的比例$\epsilon$是未知的，我们考虑如何**自适应地决定采样次数**。对于这种情形，算法从最坏的情形开始，当发现更大的一致集，则把估计更新。例如如果最坏的估计是$\epsilon=0.5$，但是一旦发现内点的一致集占数据的$80\%$，那么把估计更新为$\epsilon = 0.2$，然后重新计算最大采样次数$N$。这个自适应算法如下：

> 设总数据点的个数为$M$，初始化$N=\infin$ ，当前采样次数$n=0$
>
> 当$N > n$则重复：
>
>   1. 选取一个样本并计算其内点个数$m$
>   2. 令$\epsilon=1-m/M$
>   3. 取$p=0.99$并由$\epsilon$计算$N$
>   4. $n$加一
>
> 终止



#### 3. 一致集多大合适？

​	根据经验，在给定野点值的假定比例后，如果一致集的大小接近该数据集内点数的期望时则迭代停止。即对$n$个数据$T=(1-\epsilon)n$。



### 五、参考资料

https://blog.csdn.net/kokerf/article/details/72673268